{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chemvae.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPI49mO7gibsLDuHmvOmeJI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangsushen1992/chemical_vae/blob/master/chemvae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHzMnkp3phus",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "a07c8b64-a5f3-4421-b132-42be582f6769"
      },
      "source": [
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!time conda install -q -y -c conda-forge rdkit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-11 21:09:54--  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.201.79, 104.18.200.79, 2606:4700::6812:c84f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.201.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh [following]\n",
            "--2020-07-11 21:09:54--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "PREFIX=/usr/local\n",
            "^C\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n",
            "\n",
            "real\t0m21.461s\n",
            "user\t0m7.882s\n",
            "sys\t0m2.031s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLnft4lFpxYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glOubtfDQhB4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "79b36b8d-4931-40a0-85bf-58784404a758"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\".\")\n",
        "import os\n",
        "print(os.getcwd())\n",
        "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "import argparse\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Lambda\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.layers.core import Dense, Flatten, RepeatVector, Dropout\n",
        "from keras.layers.convolutional import Convolution1D\n",
        "from keras.layers.recurrent import GRU\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "# from chemvae import hyperparameters_1\n",
        "# from chemvae.train_vae import vectorize_data\n",
        "from keras.models import Sequential\n",
        "from keras.layers import InputLayer\n",
        "import copy\n",
        "import json\n",
        "from keras.callbacks import CSVLogger, LambdaCallback\n",
        "import yaml\n",
        "import rdkit\n",
        "import mol_utils as mu\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlukYp0-XhX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_data(params):\n",
        "    # @out : Y_train /Y_test : each is list of datasets.\n",
        "    #        i.e. if reg_tasks only : Y_train_reg = Y_train[0]\n",
        "    #             if logit_tasks only : Y_train_logit = Y_train[0]\n",
        "    #             if both reg and logit_tasks : Y_train_reg = Y_train[0], Y_train_reg = 1\n",
        "    #             if no prop tasks : Y_train = []\n",
        "\n",
        "    MAX_LEN = params['MAX_LEN']\n",
        "\n",
        "    CHARS = yaml.safe_load(open(params['char_file']))\n",
        "    params['NCHARS'] = len(CHARS)\n",
        "    NCHARS = len(CHARS)\n",
        "    CHAR_INDICES = dict((c, i) for i, c in enumerate(CHARS))\n",
        "    #INDICES_CHAR = dict((i, c) for i, c in enumerate(CHARS))\n",
        "\n",
        "    ## Load data for properties\n",
        "    if params['do_prop_pred'] and ('data_file' in params):\n",
        "        if \"data_normalization_out\" in params:\n",
        "            normalize_out = params[\"data_normalization_out\"]\n",
        "        else:\n",
        "            normalize_out = None\n",
        "\n",
        "        ################\n",
        "        if (\"reg_prop_tasks\" in params) and (\"logit_prop_tasks\" in params):\n",
        "            smiles, Y_reg, Y_logit = mu.load_smiles_and_data_df(params['data_file'], MAX_LEN,\n",
        "                    reg_tasks=params['reg_prop_tasks'], logit_tasks=params['logit_prop_tasks'],\n",
        "                    normalize_out = normalize_out)\n",
        "        elif \"logit_prop_tasks\" in params:\n",
        "            smiles, Y_logit = mu.load_smiles_and_data_df(params['data_file'], MAX_LEN,\n",
        "                    logit_tasks=params['logit_prop_tasks'], normalize_out=normalize_out)\n",
        "        elif \"reg_prop_tasks\" in params:\n",
        "            smiles, Y_reg = mu.load_smiles_and_data_df(params['data_file'], MAX_LEN,\n",
        "                    reg_tasks=params['reg_prop_tasks'], normalize_out=normalize_out)\n",
        "        else:\n",
        "            raise ValueError(\"please sepcify logit and/or reg tasks\")\n",
        "\n",
        "    ## Load data if no properties\n",
        "    else:\n",
        "        smiles = mu.load_smiles_and_data_df(params['data_file'], MAX_LEN)\n",
        "\n",
        "    if 'limit_data' in params.keys():\n",
        "        sample_idx = np.random.choice(np.arange(len(smiles)), params['limit_data'], replace=False)\n",
        "        smiles=list(np.array(smiles)[sample_idx])\n",
        "        if params['do_prop_pred'] and ('data_file' in params):\n",
        "            if \"reg_prop_tasks\" in params:\n",
        "                Y_reg =  Y_reg[sample_idx]\n",
        "            if \"logit_prop_tasks\" in params:\n",
        "                Y_logit =  Y_logit[sample_idx]\n",
        "\n",
        "    print('Training set size is', len(smiles))\n",
        "    print('first smiles: \\\"', smiles[0], '\\\"')\n",
        "    print('total chars:', NCHARS)\n",
        "\n",
        "    print('Vectorization...')\n",
        "    X = mu.smiles_to_hot(smiles, MAX_LEN, params[\n",
        "                             'PADDING'], CHAR_INDICES, NCHARS)\n",
        "\n",
        "    print('Total Data size', X.shape[0])\n",
        "    if np.shape(X)[0] % params['batch_size'] != 0:\n",
        "        X = X[:np.shape(X)[0] // params['batch_size']\n",
        "              * params['batch_size']]\n",
        "        if params['do_prop_pred']:\n",
        "            if \"reg_prop_tasks\" in params:\n",
        "                Y_reg = Y_reg[:np.shape(Y_reg)[0] // params['batch_size'] * params['batch_size']]\n",
        "            if \"logit_prop_tasks\" in params:\n",
        "                Y_logit = Y_logit[:np.shape(Y_logit)[0] // params['batch_size'] * params['batch_size']]\n",
        "\n",
        "    np.random.seed(params['RAND_SEED'])\n",
        "    rand_idx = np.arange(np.shape(X)[0])\n",
        "    np.random.shuffle(rand_idx)\n",
        "\n",
        "    TRAIN_FRAC = 1 - params['val_split']\n",
        "    num_train = int(X.shape[0] * TRAIN_FRAC)\n",
        "\n",
        "    if num_train % params['batch_size'] != 0:\n",
        "        num_train = num_train // params['batch_size'] * \\\n",
        "            params['batch_size']\n",
        "\n",
        "    train_idx, test_idx = rand_idx[: int(num_train)], rand_idx[int(num_train):]\n",
        "\n",
        "    if 'test_idx_file' in params.keys():\n",
        "        np.save(params['test_idx_file'], test_idx)\n",
        "\n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    print('shape of input vector : {}', np.shape(X_train))\n",
        "    print('Training set size is {}, after filtering to max length of {}'.format(\n",
        "        np.shape(X_train), MAX_LEN))\n",
        "\n",
        "    if params['do_prop_pred']:\n",
        "        # !# add Y_train and Y_test here\n",
        "        Y_train = []\n",
        "        Y_test = []\n",
        "        if \"reg_prop_tasks\" in params:\n",
        "            Y_reg_train, Y_reg_test = Y_reg[train_idx], Y_reg[test_idx]\n",
        "            Y_train.append(Y_reg_train)\n",
        "            Y_test.append(Y_reg_test)\n",
        "        if \"logit_prop_tasks\" in params:\n",
        "            Y_logit_train, Y_logit_test = Y_logit[train_idx], Y_logit[test_idx]\n",
        "            Y_train.append(Y_logit_train)\n",
        "            Y_test.append(Y_logit_test)\n",
        "\n",
        "        return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "    else:\n",
        "        return X_train, X_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwbnpD7TXQuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "def load_params(param_file=None, verbose=True):\n",
        "    # Parameters from params.json and exp.json loaded here to override parameters set below\n",
        "    if param_file is not None:\n",
        "        hyper_p = json.loads(open(param_file).read(),\n",
        "                             object_pairs_hook=OrderedDict)\n",
        "        if verbose:\n",
        "            print('Using hyper-parameters:')\n",
        "            for key, value in hyper_p.items():\n",
        "                print('{:25s} - {:12}'.format(key, str(value)))\n",
        "            print('rest of parameters are set as default')\n",
        "    parameters = {\n",
        "\n",
        "        # for starting model from a checkpoint\n",
        "        'reload_model': False,\n",
        "        'prev_epochs': 0,\n",
        "\n",
        "        # general parameters\n",
        "        'batch_size': 1,\n",
        "        'epochs': 1,\n",
        "        'val_split': 0.1, #validation split\n",
        "        'loss': 'categorical_crossentropy', # set reconstruction loss\n",
        "\n",
        "        # convolution parameters\n",
        "        'batchnorm_conv': True,\n",
        "        'conv_activation': 'tanh',\n",
        "        'conv_depth': 2, #4,\n",
        "        'conv_dim_depth': 8,\n",
        "        'conv_dim_width': 8,\n",
        "        'conv_d_growth_factor': 1,#1.15875438383,\n",
        "        'conv_w_growth_factor': 1,#1.1758149644,\n",
        "\n",
        "        # decoder parameters\n",
        "        'gru_depth': 2,#4,\n",
        "        'rnn_activation': 'tanh',\n",
        "        'recurrent_dim': 35,\n",
        "        'do_tgru': False,                # use custom terminal gru layer \n",
        "        'terminal_GRU_implementation': 0, # use CPU intensive implementation; other implementation modes (1 - GPU, 2- memory) are not yet implemented\n",
        "        'tgru_dropout': 0.0,\n",
        "        'temperature': 1.00,            # amount of noise for sampling the final output \n",
        "\n",
        "        # middle layer parameters \n",
        "        'hg_growth_factor': 1,#1.4928245388, # growth factor applied to determine size of next middle layer.\n",
        "        'hidden_dim': 50,#100,\n",
        "        'middle_layer': 2,\n",
        "        'dropout_rate_mid': 0.0,\n",
        "        'batchnorm_mid': True,          # apply batch normalization to middle layers\n",
        "        'activation': 'tanh',\n",
        "\n",
        "        # Optimization parameters\n",
        "        'lr': 0.000312087049936,\n",
        "        'momentum': 0.936948773087,\n",
        "        'optim': 'adam',                # optimizer to be used\n",
        "\n",
        "        # vae parameters\n",
        "        'vae_annealer_start': 22,       # Center for variational weigh annealer \n",
        "        'batchnorm_vae': False,         # apply batch normalization to output of the variational layer\n",
        "        'vae_activation': 'tanh',\n",
        "        'xent_loss_weight': 1.0,        # loss weight to assign to reconstruction error.\n",
        "        'kl_loss_weight': 1.0,          # loss weight to assing to KL loss\n",
        "        \"anneal_sigmod_slope\": 1.0,     # slope of sigmoid variational weight annealer\n",
        "        \"freeze_logvar_layer\": False,   # Choice of freezing the variational layer until close to the anneal starting epoch\n",
        "        \"freeze_offset\": 1,             # the number of epochs before vae_annealer_start where the variational layer should be unfrozen\n",
        "\n",
        "        # property prediction parameters:\n",
        "        'do_prop_pred': False,          # whether to do property prediction\n",
        "        'prop_pred_depth': 3,\n",
        "        'prop_hidden_dim': 36,\n",
        "        'prop_growth_factor': 0.8,      # ratio between consecutive layer in property prediction\n",
        "        'prop_pred_activation': 'tanh', \n",
        "        'reg_prop_pred_loss': 'mse',    # loss function to use with property prediction error for regression tasks\n",
        "        'logit_prop_pred_loss': 'binary_crossentropy',  # loss function to use with property prediction for logistic tasks \n",
        "        'prop_pred_loss_weight': 0.5,\n",
        "        'prop_pred_dropout': 0.0,\n",
        "        'prop_batchnorm': True,\n",
        "\n",
        "        # print output parameters\n",
        "        \"verbose_print\": 0,\n",
        "\n",
        "    }\n",
        "    # overwrite parameters\n",
        "    parameters.update(hyper_p)\n",
        "    return parameters\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM2S954bQzsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def layer_model(params):\n",
        "    '''\n",
        "    This function sets up the model of autoencoders. It consists of encoder layers (Conv1D) defined by 'conv_depth', \n",
        "    and decoder layers defined by 'gru_depth'. There are middle layers defined by 'middle_layer' which are fully \n",
        "    connected layers defined by 1*1 Conv1D layers.\n",
        "    '''\n",
        "    totallayer=(params['conv_depth']*2+3+params['middle_layer']*3*2+params['gru_depth'])\n",
        "    layer=[None]*totallayer\n",
        "    # ----------------- Input layer-----------------\n",
        "    layer[0] = keras.layers.InputLayer(input_shape=(params['MAX_LEN'], params['NCHARS']), name='input_molecule_smi')\n",
        "    # -----------------Encoder layers with batch normalisation-----------------\n",
        "    layer[1] = Convolution1D(int(params['conv_dim_depth'] *\n",
        "                          params['conv_d_growth_factor']),\n",
        "                      int(params['conv_dim_width'] *\n",
        "                          params['conv_w_growth_factor']),\n",
        "                      activation='tanh',\n",
        "                      name=\"encoder_conv0\")\n",
        "    if params['batchnorm_conv']:\n",
        "        layer[2] = BatchNormalization(axis=-1, name=\"encoder_norm0\")\n",
        "\n",
        "    for j in range(1, params['conv_depth']):\n",
        "        layer[1+j*2] = Convolution1D(int(params['conv_dim_depth'] *\n",
        "                            params['conv_d_growth_factor'] ** (j)),\n",
        "                            int(params['conv_dim_width'] *\n",
        "                            params['conv_w_growth_factor'] ** (j)),\n",
        "                            activation='tanh',\n",
        "                            name=\"encoder_conv{}\".format(j))\n",
        "        if params['batchnorm_conv']:\n",
        "            layer[2+j*2] = BatchNormalization(axis=-1,\n",
        "                                   name=\"encoder_norm{}\".format(j))\n",
        "    n=params['conv_depth']*2+1\n",
        "\n",
        "    # -----------------Flatten the middle layers to input to GRUS-----------------\n",
        "    layer[n]=Flatten(name='encoder_flatten')\n",
        "    #-----------------Repeat the vectors to match the length required for GRU-----------------\n",
        "    layer[n+1] = RepeatVector(params['MAX_LEN'],name='decoder_repeat')\n",
        "    n=n+2\n",
        "\n",
        "    #-----------------Decoder defined through GRU layers -----------------\n",
        "    if params['gru_depth'] > 1:\n",
        "        layer[n] = GRU(params['recurrent_dim'],\n",
        "                    return_sequences=True, activation='tanh',\n",
        "                    name=\"decoder_gru0\")\n",
        "\n",
        "        for k in range(params['gru_depth'] - 2):\n",
        "            layer[n+1+k] = GRU(params['recurrent_dim'],\n",
        "                        return_sequences=True, activation='tanh',\n",
        "                        name=\"decoder_gru{}\".format(k + 1))\n",
        "\n",
        "        if params['do_tgru']:\n",
        "            layer[n+params['gru_depth']-1] = TerminalGRU(params['NCHARS'],\n",
        "                                rnd_seed=params['RAND_SEED'],\n",
        "                                recurrent_dropout=params['tgru_dropout'],\n",
        "                                return_sequences=True,\n",
        "                                activation='softmax',\n",
        "                                temperature=0.01,\n",
        "                                name='decoder_tgru',\n",
        "                                implementation=params['terminal_GRU_implementation'])([x_dec, true_seq_in])\n",
        "        else:\n",
        "            layer[n+params['gru_depth']-1]= GRU(params['NCHARS'],\n",
        "                        return_sequences=True, activation='softmax',\n",
        "                        name='decoder_gru_final')\n",
        "\n",
        "    else:\n",
        "        \n",
        "        layer[n+1] = GRU(params['NCHARS'],\n",
        "                    return_sequences=True, activation='softmax',\n",
        "                    name='decoder_gru_final'\n",
        "                    )\n",
        "    #-----------------Return the model as a list of different layers objects-----------------\n",
        "    return layer\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps9kEM-NXmn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Cascade(params, training_layer, X_train, Y_train, X_test, Y_test,epochs=20,loss='categorical_crossentropy',\n",
        "                        #optimizer='sgd',initialLr=0.01,\n",
        "                        weightDecay=10e-4,patience=10,\n",
        "                        windowSize=5,batch_size=128,outNeurons=64,nb_classes=10,index=0,fast=True,gradient=False):\n",
        "    '''\n",
        "    This function performs cascade learning to the autoencoders defined in the argument training_layer\n",
        "    '''\n",
        "    #-----------------Define key variables-----------------\n",
        "    nextModelToTrain = list() \n",
        "    saveEncoderLayersIndexes = list() \n",
        "    saveDecoderLayersIndexes = list()\n",
        "    saveFCEncoderLayersIndexes = list() \n",
        "    saveFCDecoderLayersIndexes = list()\n",
        "    history = dict()\n",
        " \n",
        "    #-----------------Append layer number of key layers (convolutioinal layers) to become a list -----------------\n",
        "    # This is necessary because we treat batchnormalisation and dropout layers as an integral to convolutional layers\n",
        "    # Each time a convolutional layer is added, the batchnormalisation and dropout layers are added together.\n",
        "    # Therefore, saveEncoderLayerIndexes for example, keeps track of the layer number of all convoutional layers such that\n",
        "    # trianing is performed with the addition of a whole block of layers.\n",
        "    i = 0\n",
        "    print(training_layer)\n",
        "    exit()\n",
        "    for currentLayer in training_layer: \n",
        "        if (currentLayer.name[0:12] == 'encoder_conv'):\n",
        "            saveEncoderLayersIndexes.append(i)\n",
        "        if (currentLayer.name[0:15] == 'encoder_flatten'):\n",
        "            saveFCEncoderLayersIndexes.append(i)\n",
        "        if (currentLayer.name[0:14] == 'decoder_repeat'):\n",
        "            saveFCDecoderLayersIndexes.append(i)\n",
        "        if (currentLayer.name[0:11] == 'decoder_gru'):\n",
        "            saveDecoderLayersIndexes.append(i)\n",
        "        i += 1\n",
        "\n",
        "\n",
        "    #-----------------The model starts to append blocks of layers-----------------\n",
        "    for i in range(0,len(saveEncoderLayersIndexes)): \n",
        "        if ('iter' + str(i) not in history.keys()): \n",
        "            history['iter' + str(i)] = dict() \n",
        "\n",
        "        if(i == 0):\n",
        "            for j in training_layer[0:saveEncoderLayersIndexes[1]]: #FOR CORRESPONDING LAYERS FOR CURRENT RUN IN MODEL\n",
        "                nextModelToTrain.append(j)\n",
        "            for j in training_layer[saveFCEncoderLayersIndexes[0]:saveFCDecoderLayersIndexes[0]]: #FOR CORRESPONDING LAYERS FOR CURRENT RUN IN MODEL\n",
        "                nextModelToTrain.append(j)\n",
        "       \n",
        "            for j in training_layer[saveFCDecoderLayersIndexes[0]:saveDecoderLayersIndexes[0]]: #FOR CORRESPONDING LAYERS FOR CURRENT RUN IN MODEL\n",
        "                nextModelToTrain.append(j)\n",
        "            for j in training_layer[saveDecoderLayersIndexes[-1]:saveDecoderLayersIndexes[-2]:-1]: #FOR CORRESPONDING LAYERS FOR CURRENT RUN IN MODEL\n",
        "                nextModelToTrain.append(j)\n",
        "\n",
        "        else:\n",
        "\n",
        "            for k in training_layer[saveEncoderLayersIndexes[i]+1:saveEncoderLayersIndexes[i]-1:-1]: #GET THE LAYERS OF NEXT MODEL TO TRAIN\n",
        "                    nextModelToTrain.insert(1+i*2,k)\n",
        "            j = training_layer[saveDecoderLayersIndexes[-i-1]] #GET THE LAYERS OF NEXT MODEL TO TRAIN\n",
        "            nextModelToTrain.insert(-i,j)\n",
        "\n",
        "        nextModelToTrainInputs = Input(shape=(params['MAX_LEN'], params['NCHARS']), name='input_smi')\n",
        "        \n",
        "        #-----------------Starts to add layers to the sequential model-----------------\n",
        "\n",
        "        ModelToTrain=Sequential()\n",
        "   \n",
        "        for index,currentlayer in enumerate(nextModelToTrain[:]):\n",
        "            print('Currently adding',currentlayer)\n",
        "            ModelToTrain.add(currentlayer)\n",
        "        \n",
        "        #-----------------Setting the newly added layers to be trainable -----------------\n",
        "        #-----------------and the already present layers to be not trainable-----------------\n",
        "        if i==0:\n",
        "            for layer in ModelToTrain.layers:\n",
        "                layer.trainable= True\n",
        "        else:\n",
        "            for layer in ModelToTrain.layers[i:i*2]:\n",
        "                layer.trainable=False\n",
        "            for layer in ModelToTrain.layers[i*2+2:-i-1]:\n",
        "                layer.trainable=False\n",
        "            for layer in ModelToTrain.layers[-i:]:\n",
        "                layer.trainable=False\n",
        "\n",
        "        #-----------------Compiling the model-----------------\n",
        "        optimizer = keras.optimizers.Adam(lr=0.01)\n",
        "        ModelToTrain.compile(loss=loss,optimizer=optimizer,metrics=['accuracy'])\n",
        "\n",
        "        model_train_targets = {'x_pred':X_train,\n",
        "                              'z_mean_log_var':np.ones((np.shape(X_train)[0], params['hidden_dim'] * 2))}\n",
        "        model_test_targets = {'x_pred':X_test,\n",
        "                              'z_mean_log_var':np.ones((np.shape(X_test)[0], params['hidden_dim'] * 2))}\n",
        "        \n",
        "        # ----------------- Fitting of the model begins here-----------------\n",
        "        print('Fitting begins .....')\n",
        "        csv_logger = CSVLogger('training.csv',append=True)\n",
        "        tmpHistory=ModelToTrain.fit(X_train, X_train,\n",
        "                            batch_size=params['batch_size'],\n",
        "                            epochs=params['epochs'],\n",
        "                            initial_epoch=params['prev_epochs'],\n",
        "                            callbacks=[csv_logger],\n",
        "                            verbose=params['verbose_print'],\n",
        "                            validation_data=[X_test, X_test])\n",
        "        \n",
        "        #-----------------Printing key results of the model-----------------\n",
        "        print('Weigths are', ModelToTrain.get_weights())\n",
        "\n",
        "        print(ModelToTrain.summary())\n",
        "        history['iter'+str(i)]['lossTraining'] = tmpHistory.history['loss']\n",
        "        history['iter'+str(i)]['accuracyTraining'] = tmpHistory.history['acc']\n",
        "       \n",
        "        print('iter',str(i), history['iter'+str(i)])\n",
        "        \n",
        "    return ModelToTrain, history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1Vw7vnwXs5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main_run(params):\n",
        "    X_train, X_test, Y_train, Y_test = vectorize_data(params)\n",
        "    auto_layer = layer_model(params)\n",
        "    print('Model is',auto_layer)\n",
        "    model, history = Cascade(params, auto_layer, X_train, Y_train, X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3N6Es6BmfGP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "cb5905ea-5bcd-44d8-b670-3d688eba385c"
      },
      "source": [
        "params = load_params('/content/exp_1.json')\n",
        "main_run(params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-22bdfd1605e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/exp_1.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmain_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_params' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNLhsegzraEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}